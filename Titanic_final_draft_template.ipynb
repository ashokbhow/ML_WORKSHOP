{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Jupyter shortcuts <br>\n- shift + enter run cell, select below.<br>\n- ctrl + enter run cell.<br>\n- option + enter run cell, insert below.<br>\n- a insert cell above.<br>\n- b insert cell below.<br>\n- c copy cell.<br>\n- v paste cell.<br>\n- dd delete selected cell.<br>\n- m-markdown\n- Enter - Go into Edit Mode\n- ii - Interrupt kernel\n- oo - Restart kernel\n- shift+tab within braces - help on parameters ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a name='top' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Go to <a href=#numpanda>Numpy and Pandas</a>\n- Go to <a href=#prepdata>Preparing data</a>\n- Go to <a href=#dataexplore>Exploratory data analysis</a>\n- Go to <a href=#Featureeng>Feature Engineering</a>\n- Go to <a href=#Featureanalysis>Feature Analysis and Selection </a>\n- Go to <a href=#modeling>Model Building</a>\n- Go to <a href=#confusionmat>Confusion Matrix</a>\n- Go to <a href=#fit>Overfitting and Underfitting</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Python basics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# python print"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# single/multi line comment\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# data type/casting/type"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# string index\n# 0 based indexing\n\n# char of 13th position\n# reverse index, last character\n# from 0 to 4 for index. last value excluded 0-3.\n# last 4 character of str"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# string Operation\nA=\"Thriller is the sixth studio album\"\nprint(\"befor upper:\",A)\nB=#upper()\nprint(\"After upper:\",B)\n\nA=\"Michael Jackson is the best\"\nB=# replace('Michael', 'Janet')\nprint(B)\n\nName=\"Michael Jackson\"\n# find('el')\nprint (Name)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# function\ndef type_of_album(artist,album,year_released):\n    if year_released > 1980:\n        print(artist,album,year_released)\n        return \"Modern\"\n    else:\n        print(artist,album,year_released)\n        return \"Oldie\"\n    \nx = type_of_album(\"Michael Jackson\",\"Thriller\",1980)\nprint(x)"
        }, 
        {
            "source": "<a name='numpanda' />\n<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# NumPy and Pandas<br>\nNumPy: <br>\n- Open source library\n- Working with high-performance arrays and matrices<br>\n- Mathematical operations are vectorized<br>\n- Main object is ndarray<br>\n- Heavy computations (3+ dimension)<br>\n- Arrays are accessed by their integer position, starting with zero for the first element<br>\n\nPandas: <br>\n- A package built around Numpy for data manipulation that uses the DataFrame objects<br>\n- Series is the primary building block of pandas<br>\n- Series Object is more flexible as you can define your own labeled index and access elements of an array<br>\n- Aligning data from different Series and matching labels (joining) with Series objects is more efficient. For example dealing with missing values. If there are no matching labels during alignment, pandas returns NaN (not any number) so that the operation does not fail.<br>\n\nNote:\n- Query-like operations works better in pandas as it has relational database concept\n- Data manupulination is easier in pandas\n- Numpy consumes less memory compared to pandas\n- Numpy generally performs better than pandas for 50K rows or less\n- Pandas generally performs better than numpy for 500K rows or more\n- For 50K to 500K rows, it is a toss up between pandas and numpy depending on the kind of operation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np\nimport pandas as pd\nprint(np.__version__)\n\nprint (a1)\nprint (a2)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# loading/viewing data\n\n# csv_path='https://ibm.box.com/shared/static/keo2qz0bvh4iu6gf5qjq4vdrkt67bvvb.csv'\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# display first 5 rows"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# show values from last"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# show sample values from dataframe"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# !pip install xlrd\nxlsx_path='https://ibm.box.com/shared/static/mzd4exo31la6m7neva2w45dstxfg5s86.xlsx'\n\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# print columns\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# print datatypes\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# we can also see how much space is taken up\n\n# here is the size (num rows, num cols)\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# data selection\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# value_counts\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# multiple column selection\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(df.iloc[0,0]) # 1st row and first column\nprint(df.iloc[1,0]) # 2nd row and first column"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# slicing df\ndf.iloc[0:3, 0:3] # last index is exclusive"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# column value based selection\ndf.set_index(\"Album\", inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Select value of indexed column\na=df.loc['Thriller']\na"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Select multiple values of indexed column\nb=df.loc[['Thriller','Back in Black']]\nb"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#reset index in dataframe"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# conditional selection\nc=df.loc[df['Artist']=='Pink Floyd']\nc"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# conditional selection of specific columns\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Update column value on condition"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# add new column to dataframe based on conditions\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# group by\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# multi column group by\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# dataframe copy\n"
        }, 
        {
            "source": "# Problem Statement: Predict Survival on the Titanic data set <br>\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\nPractice Skills\n    \n    Python basics\n    Binary classification\n\n\nWORKFLOW: import libraries-> loading data-> cleaning up data-> analyzing data -> feature engineering & selection -> model building -> model evaluation ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "#### CELL TO UPDATE\nAs we work through this notebook, you'll see some notebook cells (a block of either code or text) that has \"CELL TO UPDATE\" written in it. These are exercises for you to do to help cement your understanding of the concepts we're talking about. Once you've written the code to answer a specific question, you can run the code by clicking inside the cell (box with code in it) with the code you want to run and then hit CTRL + ENTER (CMD + ENTER on a Mac). ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 1. Importing libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['patch.force_edgecolor'] = True\nplt.style.use('seaborn')\n# !pip install seaborn\nimport seaborn as sns\n\n\n# from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')"
        }, 
        {
            "source": "# 2. Loading data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Import train and test dataset from local/watson studio"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nprint(\"Dimensions of train: {}\".format(df_train.shape))\nprint(\"Dimensions of test: {}\".format(df_practice.shape))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# display first five rows\ndf_train.head(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# explore data types of different columns in the dataframe.\ndf_train.dtypes"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# navigate and explore 10 sample rows from the test dataframe and compare its schema with the training dataset\ndf_practice.sample(10)"
        }, 
        {
            "source": "<a name='prepdata' />\n<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 3. Preparing data\nHere's what we're going to do today:\n\n    * Take a first look at the data\n    * See how many missing data points we have\n    * Dealing with missing values", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Handling Columns with Missing Values\n\n### See how many missing data points we have in the Training set\nWe will show 2 ways of doing this.\n\nWe can use seaborn to create a simple heatmap to see where we are missing data!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sns.heatmap(df_train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
        }, 
        {
            "source": "Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# check for missing values - seeing counts\nfor c in df_train.columns:\n    print(c,np.sum(df_train[c].isnull()))"
        }, 
        {
            "source": "There are different ways of dealing with missing values: <br>\n\n (1) replace the missing values with some other values.<br>\n  - mean of the rest\n  - median per group\n  - higher frequency count<br>\n  \n(2) dropping the column, <br>\n (3) dropping rows. <br>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Filling in missing age values with mean\n\nWe want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ndf_train['Age'].fillna(df_train['Age'].mean(), inplace=True)\nsns.heatmap(df_train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
        }, 
        {
            "source": "### Replace missing values using higher frequency count", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfreq_port = df_train.Embarked.dropna().mode()[0]\nfreq_port"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ndf_train['Embarked'] = df_train['Embarked'].fillna(freq_port)"
        }, 
        {
            "source": "### Drop a column where majority of the values are missing - e.g. Cabin\nCabin column is missing 687 values out of 891", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ndf_train.drop(['Cabin'], axis=1, inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# dropping any empty rows\ndf_train.dropna(inplace=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nsns.heatmap(df_train.isnull(), yticklabels=False, cbar=False, cmap='viridis')"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Converting Categorical Variables\n\nWe'll need to convert categorical variables to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.\n* Example: Let's see an example in the Sex column. Machine Learning algorithm will not be able to take in just Male / Female as a value. \n* Mathematical formula: This is because the Machine Learning algorithm will try to come up with a Mathematical formual where all the input values need to be numbers, not text. \n* What to do: We'll have to create a new column with 0 or 1 value to indicate if a person is Male or not. Encoding the variable in a way that the Machine Learning algorithm can use. This is known as creating dummy variable. \n* Another example: Embarked column, which is a letter representing a city. ML algorithm can't take in the letters and do something with them.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# explore unique values of any column\ndf_train['Sex'].unique()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# print the top 5 rows with Sex column\ndf_train['Sex'].head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# Multicollinearity: one column (feature) is a perfect predictor for the other column (feature). If we feed both these columns\n# to Machine Learning Algorithm, it will immediately know that it can predict the value of one column from the other column here. This issue is called\n# Multicollinearity, which will mess up the algorithm because a bunch of columns will be a perfect predictors for other columns. \n\n# To fix for Multicollinearity when using get_dummies you need to add another argument to the get_dummies method: drop_first=True. Otherwise get_dummies will create \n# two columns that will be perfect predictors of each other.\n\npd.get_dummies(df_train['Sex'], drop_first=True).head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# we'll set this to sex as a new dataframe\nsex = pd.get_dummies(df_train['Sex'], drop_first=True)\nembarked = pd.get_dummies(df_train['Embarked'], drop_first=True,prefix='Embarked')\nPclass = pd.get_dummies(df_train['Pclass'], drop_first=True,prefix='Pclass')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# add these new columns to the df_train dataframe\ndf_train = pd.concat([df_train, sex, embarked,Pclass], axis=1)\ndf_train.head()"
        }, 
        {
            "source": "## Exercise Set 1", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: use seaborn to create a simple heatmap to see where we are missing data in the practice set\n"
        }, 
        {
            "source": "\n<div align=\"right\">\n<a href=\"#String2\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n</div>\n<div id=\"String2\" class=\"collapse\">\n <a ><img src = \"https://ibm.box.com/shared/static/pf9bc65mmihgmm57c4jrv6ykn35ak98k.png\" width = 500, align = \"center\"></a>\n```\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: count the number of missing values in the practice set\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#String1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the expected output</a>\n\n</div>\n<div id=\"String1\" class=\"collapse\">\n\n```\nPassengerId 0\nPclass 0\nName 0\nSex 0\nAge 86\nSibSp 0\nParch 0\nTicket 0\nFare 1\nCabin 327\nEmbarked 0\n\n```\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: Fill in the missing Age values in the practice dataset with the mean value from the training set's Age column\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: Fill in the missing Embark values in the practice dataset with the freq_port\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: Drop the Cabin column from the practice set\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: Drop any empty rows\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# applying same to the practice set to use later.\nsex_test=pd.get_dummies(df_practice['Sex'], drop_first=True)\nEmbarked_test=pd.get_dummies(df_practice['Embarked'], drop_first=True,prefix='Embarked')\nPclass_test=pd.get_dummies(df_practice['Pclass'], drop_first=True,prefix='Pclass')\ndf_practice = pd.concat([df_practice, sex_test, Embarked_test,Pclass_test], axis=1)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL to UPDATE\n# Task: see the first couple of rows of df_practice after the modifications\n"
        }, 
        {
            "source": "\n<div align=\"right\">\n<a href=\"#String3\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the expected output</a>\n</div>\n<div id=\"String3\" class=\"collapse\">\n\n <a ><img src = \"https://ibm.box.com/shared/static/9umdiorvu1exphmg1ms5jas932exdsdv.png\" width = 1100, align = \"center\"></a>\n\n</div>\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n## 4. Exploratory data analysis\n<a name='dataexplore' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# Ratio of survival Vs dead in the dataset\nplt.figure(figsize=(6,4))\nfig, ax = plt.subplots()\ndf_train.Survived.value_counts().plot(kind='barh', color=\"blue\", alpha=.65)\nax.set_ylim(-1, len(df_train.Survived.value_counts())) \nplt.title(\"Survival Breakdown (1 = Survived, 0 = Died)\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# explore survival ration for different Embarked locations\nsurvived_embark = df_train[df_train['Survived']==1]['Embarked'].value_counts()\ndead_embark = df_train[df_train['Survived']==0]['Embarked'].value_counts()\ndf = pd.DataFrame([survived_embark,dead_embark])\ndf.index = ['Survived','Dead']\ndf.plot(kind='bar', stacked=True, figsize=(15,8))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# Based on Domain knowledge we know women and children were given preference to lifeboats (as they were in real life)\nsex_pivot = df_train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot.plot.bar()\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ng = sns.FacetGrid(df_train, col='Survived',size=4,aspect=1.2)\ng.map(plt.hist, 'Age', bins=20)"
        }, 
        {
            "source": "**Observations.**\n\n- Infants (Age <=4) had high survival rate.\n- Oldest passengers (Age = 80) survived.\n- Large number of 15-25 year olds did not survive.\n- Most passengers are in 15-35 age range.\n\n**Decisions.**\n\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n- We should consider Age in our model training.\n- Complete the Age feature for null values.\n- We should band age groups.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ngrid = sns.FacetGrid(df_train, row='Pclass', col='Survived', size=2.5, aspect=2)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()"
        }, 
        {
            "source": "**Observations.**\n\n- Pclass=3 had most passengers, however most did not survive. \n- Infant passengers in Pclass=2 and Pclass=3 mostly survived.\n- Most passengers in Pclass=1 survived. \n- Pclass varies in terms of Age distribution of passengers.\n\n**Decisions.**\n\n- Consider Pclass for model training.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nimport pixiedust"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "pixiedust": {
                    "displayParams": {
                        "handlerId": "barChart", 
                        "valueFields": "Age", 
                        "keyFields": "Embarked", 
                        "aggregation": "COUNT"
                    }
                }
            }, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ndisplay(df_train)"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# Sex column - no more needed as we have encoded it in the Male column\n# Embarked column - no more needed\n# Also, ticket column and Name column are not needed, not much information to use. \n# Let's drop them\n# explain why dropping them\ndf_train.drop(['Sex','Embarked','Pclass','Ticket'],axis=1,inplace=True)\ndf_train.head()\n# all data are numerical, no missing values, perfect for analysis and Machine Learning"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# drop from test as well\ndf_practice.drop(['Sex','Embarked','Pclass','Ticket'],axis=1,inplace=True)\ndf_practice.head()"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 5. Feature engineering\n<a name='Featureeng' />", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "## Changing integer features: Age", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# creating AgeBand column by converting Age to 5 age ranges\ndf_train['AgeBand'] = pd.cut(df_train['Age'], 5)\n# comparing survival rate between AgeBands\ndf_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# converting Age from continuous to categorical based on the AgeBand endpoints\ndf_train.loc[(df_train['Age'] <= 16), 'Age'] = 0\ndf_train.loc[(df_train['Age'] > 16) & (df_train['Age'] <= 32), 'Age'] = 1\ndf_train.loc[(df_train['Age'] > 32) & (df_train['Age'] <= 48), 'Age'] = 2\ndf_train.loc[(df_train['Age'] > 48) & (df_train['Age'] <= 64), 'Age'] = 3\ndf_train.loc[(df_train['Age'] > 64), 'Age'] = 4"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# dropping AgeBand column since it is no more needed\ndf_train = df_train.drop(['AgeBand'], axis=1)\n# changing Age column type to integer\ndf_train['Age'] = df_train['Age'].astype(int)\n# showing the first few rows of df_train \ndf_train.head()"
        }, 
        {
            "source": "## Changing float features: Fare", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# creating FareBand column by converting Fare to 4 fare ranges\ndf_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\n# comparing survival rate between FareBands\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nquantile_list = [0, .25, .5, .75, 1.]\nquantiles = df_train['Fare'].quantile(quantile_list)\nplt.rcParams['figure.figsize'] = (20,5)\n\nfig, ax = plt.subplots()\ndf_train['Fare'].hist(bins=30, grid=False)\nfor quantile in quantiles:\n    qvl = plt.axvline(quantile, color='r')\nax.legend([qvl], ['Quantiles'])\nax.set_title('Fare Histogram with Quantiles')\nax.set_xlabel('Fare')\nax.set_ylabel('Frequency')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# converting Fare from continuous to categorical based on the FareBand endpoints\ndf_train.loc[(df_train['Fare'] <= 7.91), 'Fare'] = 0\ndf_train.loc[(df_train['Fare'] > 7.91) & (df_train['Fare'] <= 14.454), 'Fare'] = 1\ndf_train.loc[(df_train['Fare'] > 14.454) & (df_train['Fare'] <= 31), 'Fare']   = 2\ndf_train.loc[(df_train['Fare'] > 31), 'Fare'] = 3"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# dropping FareBand column since it is no more needed\ndf_train = df_train.drop(['FareBand'], axis=1)\n# changing Fare column type to integer\ndf_train['Fare'] = df_train['Fare'].astype(int)\n# showing the first few rows of df_train \ndf_train.head()"
        }, 
        {
            "source": "## Exercise Set 2", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: convert Age from continuous to categorical based on the AgeBand endpoints in df_practice\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: change Age column type to integer in the df_practice\n# Task: see first few rows of the df_practice dataframe after changes done\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#String21\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for expected output</a>\n</div>\n<div id=\"String21\" class=\"collapse\">\n <a ><img src = \"https://ibm.box.com/shared/static/zvpw9txire64nnncdush9uobnxgncd2h.png\" width = 1000, align = \"center\"></a>\n\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: convert Fare from continuous to categorical based on the FareBand endpoints in df_practice\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO UPDATE\n# Task: change Fare column type to integer in the df_practice\n# Task: see first few rows of the df_practice dataframe after changes done\n"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#String22\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for expected outout</a>\n</div>\n<div id=\"String22\" class=\"collapse\">\n <a ><img src = \"https://ibm.box.com/shared/static/tn9gxzt33ejjnewauswoedpe0gjfccvf.png\" width = 1000, align = \"center\"></a>\n\n</div>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# 6. Feature Analysis & Selection\n<a name='Featureanalysis' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Two features (variables) may or may not have some relationship.\n- Chi2 Statistical Test tells whether two variables are dependent or independent (i.e. whether there exists any relationship).\n- If they are related then the relationship is either linear or non-linear.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "![Image of Yaktocat](http://givingbacksocialfund.com/wp-content/uploads/2018/09/linear-and-nonlinear-functions-worksheet-lovely-28-linear-relationships-worksheet-of-linear-and-nonlinear-functions-worksheet.gif)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Correlation coefficient measures the strength and the direction of the linear relationship between variables.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "![Image of Yaktocat](https://kaiserscience.files.wordpress.com/2015/01/correlation-levels.gif)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- None of these measurements imply causality! Mathematics alone cannot determine causality!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Analysis of Correlation Coefficients", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# let's look at data types\ndf_train.dtypes"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# let's see whether there is some correlation (linear relationship) between survival rate and each feature\n# Name, PassengerId features are omitted since they are unique to each passenger\ntarget='Survived'\nfeat=['Age','SibSp','Parch','Fare','male','Embarked_Q','Embarked_S','Pclass_2','Pclass_3']\nfor col in feat:\n    print(\"Correlation of Survival with {0:>10s}: {1:5.2f}\".format(col, df_train[target].corr(df_train[col])))"
        }, 
        {
            "source": "## Univariate Selection", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# importing required functions\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# Name, PassengerId features are omitted since they are unique to each passenger\nfeatures = df_train[feat].copy()\n# let's select 4 features that will give us the most information about the survival rate\nK = 4\nfeature_select = SelectKBest(score_func=chi2, k=K)\nfeature_select_fit = feature_select.fit(features, df_train['Survived'])\n# summarizing scores\nindices = np.argsort(feature_select_fit.scores_)[::-1][:K]\nselected_features = feature_select_fit.transform(features)\nprint(features.columns[indices])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# plotting\nplt.rcParams['figure.figsize'] = (15,4)\nplt.bar(features.columns, feature_select_fit.scores_, width=.3)\nplt.title(\"Relationship between each feature and survival\")\nplt.xlabel('Feature name')\nplt.ylabel('Chi2 score')"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a name='modeling' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "\n# 7. Model building\n\n## Decison Tree Classifier\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# Decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\nall_X = features\nall_y = df_train['Survived']\ndecision_tree.fit(all_X,all_y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\na=dict(zip(features.columns, decision_tree.feature_importances_))\ndf = pd.DataFrame.from_dict(a, orient='index').reset_index()\\\n                        .set_axis(['Features','Importance'], axis=1, inplace=False)\\\n                        .sort_values(by='Importance', ascending=False)\\\n                        .reset_index(drop=True)\n                        \n                       \ndf"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "feat_importances = pd.Series(decision_tree.feature_importances_, index=features.columns)\nfeat_importances.nlargest(20).plot(kind='barh').invert_yaxis()"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Split data for training (80%) and testing (20%)\n- training data used to define the model\n- testing data is kept to one side when the model is defined and applied to the model to validate it\n- testing data is a proxy for new data in models that need to work with as-yet unseen input", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfrom sklearn.model_selection import train_test_split\n\nall_X = features\nall_y = df_train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.20,random_state=0)"
        }, 
        {
            "source": "## Predict on test data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\ndecision_tree.fit(train_X, train_y)\npredictions = decision_tree.predict(test_X)"
        }, 
        {
            "source": "# 8. Model evaluation/Validation", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Check model test accuracy", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# actual survival is in test_y\n# predicted survival is in predictions\n# use accuracy_score method to get and display the accuracy of the predictions\n\nfrom sklearn.metrics import *\naccuracy = accuracy_score(test_y, predictions)\naccuracy"
        }, 
        {
            "source": "<a name='confusionmat' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Confusion matrix\n- TN is the number of correct predictions that an instance is negative\n- FP is the number of incorrect predictions that an instance is positive\n- FN is the number of incorrect  predictions that an instance is negative\n- TP is the number of correct predictions that an instance is positive\n- Accuracy:(AC) is the proportion of the total number of predictions that were correct. It is determined using the equation: \n    AC= TN+TP/(TN+FP+FN+TP)\n- Recall or true positive rate (TP) is the proportion of positive cases that were correctly identified, as calculated using the equation: R= TP/(FN+TP)\n- Precision(P) is the proportion of the predicted positive cases that were correct, as calculated using the equation:P= TP/(FP+TP) ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "![Image of Yaktocat](https://skappal7.files.wordpress.com/2018/08/confusion-matrix.jpg?w=748)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfrom sklearn.metrics import confusion_matrix\ncfmap=confusion_matrix(y_true=test_y,  # True labels\n                         y_pred=predictions)\n\nlabel = [\"Died\", \"Survived\"]\nsns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\nplt.xlabel(\"Prediction - logistic regression Algorithm\")\nplt.title(\"Confusion Matrix for logistic regression Algorithm\")\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# view summary of common classification metrics\nprint(classification_report(y_true=test_y,\n                              y_pred=predictions))"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Note: Our goal is to train the model to predict future data. How do we know if our model is good or bad to predict unseen future instances?\n\n## Overfitting\n- Model doesn\u2019t generalize well from our training data to unseen data.<br>\n\nHow to Know: <br>\n- Model does much better on the training set than on the test set.\n- High Variance: learning curve shows a large gap between training and test set errors, so the algorithm is suffering from high variance.\n\nPrevention options:<br>\n\n- Cross-validation\n- Train with more data\n- Remove features\n- Early stopping\n- Regularization\n\n## Underfitting\n\n- Underfitting occurs when a model is too simple \u2013 informed by too few features or regularized too much \u2013 which makes it inflexible in learning from the dataset.\n\nHow to Know: <br>\n- Poor performance on the training data and poor generalization to other data.\n- High Bias: learning curve shows high error on both the training and test sets, so the algorithm is suffering from high bias \n\nPrevention options:<br>\n- Add more Features\n- Add more parameters", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a name='fit' />", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## Cross validation to evaluate model performance", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfrom sklearn.model_selection import cross_val_score\n\n# Data  training set is divided into certain number of folds\n# in each rotation, one section will be kept as holdout set and it will train based on the rest.\n# once training is done, it will predict on the holdout set and give accuracy\n# cycle will repeat for total number of folds.\n# at the end it will take average of all the accuracy scores from different folds.\n\nscores = cross_val_score(decision_tree, all_X, all_y, cv=10)\n# scores.sort()\naccuracy = scores.mean()\n\nprint(scores)\nprint(accuracy)"
        }, 
        {
            "source": "<div align=\"right\">\n<a href=\"#String20\" class=\"btn btn-default\" data-toggle=\"collapse\">Example of CV process</a>\n</div>\n<div id=\"String20\" class=\"collapse\">\n <a ><img src = \"https://ibm.box.com/shared/static/rtmq9r034la870lssyxqqeebjaq6d50u.png\" width = 500, align = \"center\"></a>\n```\n```\n</div>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#%%capture\n!pip install mlxtend"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfrom mlxtend.plotting import plot_learning_curves\nplot_learning_curves(train_X, train_y, test_X, test_y, decision_tree)"
        }, 
        {
            "source": "## Hyperparameter Tuning", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# find out the training and testing accuracy using accuracy_score() method.\n# If there is high difference between them the model tends to overfitting situation\n\nall_X = features\nall_y = df_train['Survived']\ndecision_tree = DecisionTreeClassifier()\n\n# max_depth: This indicates how deep the tree can be\n# min_samples_split: This represents the minimum number of samples required to split an internal node\n# min_samples_leaf: This indicates minimum number of samples required to be at a leaf node\n\n# decision_tree = DecisionTreeClassifier(max_depth=10,min_samples_split=5,min_samples_leaf=5)\n\ntrain_X, test_X, train_y, test_y = train_test_split(\n    all_X, all_y, test_size=0.20,random_state=42)\n\ndecision_tree.fit(train_X, train_y)\nY_pred = decision_tree.predict(test_X)\n\ntestaccuracy = accuracy_score(test_y, Y_pred)\nprint(\"Test Accuracy: {}\".format(testaccuracy))\n\n# Training accuracy\ntrainpredictions = decision_tree.predict(train_X)\ntrainaccuracy = accuracy_score(train_y, trainpredictions)\nprint(\"Train Accuracy: {}\".format(trainaccuracy))\n# false_positive_rate, true_positive_rate, thresholds = roc_curve(test_y,Y_pred)\n# roc_auc = auc(false_positive_rate, true_positive_rate)\n# print(roc_auc)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# plot learning curve  to visualize high variance and overfitting.\n# !pip install mlxtend\nfrom mlxtend.plotting import plot_learning_curves\nplot_learning_curves(train_X, train_y, test_X, test_y, decision_tree)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfeatures.columns"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Let us predict on the unseen data  ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# lr = LogisticRegression()\n# lr.fit(all_X,all_y)\ncolumns=['Age', 'SibSp', 'Parch', 'Fare', 'male', 'Embarked_Q', 'Embarked_S',\n       'Pclass_2', 'Pclass_3']\nunseen_predictions = decision_tree.predict(df_practice[columns])\nunseen_predictions_prob=decision_tree.predict_proba(df_practice[columns])#give confidence score of predictions"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# check some sample predictions:\ndf_test_ids = df_practice[\"PassengerId\"]\ndf_test_name = df_practice[\"Name\"]\nsubmission_df = {\"PassengerId\": df_test_ids,\n                 \"Name\": df_test_name,\n                 \"Survived\": unseen_predictions,\n                \"Survived_prob_confidence\":unseen_predictions_prob[:,1]}\nsubmission = pd.DataFrame(submission_df)\nsubmission[submission['Survived']==1].head(5)"
        }, 
        {
            "source": "# Will you survive the Titanic?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Age \t SibSp  \t Parch  \t Fare \t male \t Embarked_Q  \t Embarked_S  \t Pclass_2 \t Pclass_3\n# CELL TO LEAVE UNMODIFIED\nAge = int(input(prompt=\"Age options are 0, 1, 2, or 3:\"))\nSibSp = int(input(prompt=\"SibSp options are 0, 1, 2, 3, 4, 5, 8:\"))\nParch = int(input(prompt=\"Parch options are 0, 1, 2, 3, 4, 5, 6:\"))\nFare = int(input(prompt=\"Fare options are 0, 1, 2, or 3:\"))\nmale = int(input(prompt=\"Gender options (Male): Yes - 1, No - 0:\"))\nEmbarked_Q = int(input(prompt=\"Queenstown: Yes - 1, No - 0:\"))\nEmbarked_S = int(input(prompt=\"Southampton: Yes - 1, No - 0:\"))\nPclass_2 = int(input(prompt=\"Second class ticket: Yes - 1, No - 0:\"))\nPclass_3 = int(input(prompt=\"Third class ticket: Yes - 1, No - 0:\"))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# creating new testing dataset of your data\nmy_data = {'Age': [Age],'SibSp': [SibSp],'Parch': [Parch],'Fare':[Fare],'male': [male] ,'Embarked_Q': [Embarked_Q],'Embarked_S': [Embarked_S]\n           ,'Pclass_2': [Pclass_2],'Pclass_3': [Pclass_3]\n           }\nmy_X = pd.DataFrame(data=my_data)\nmy_X"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nif decision_tree.predict(my_X) == np.array(1):\n    print(\"You survided!\")\nelse: \n    print(\"RIP\")"
        }, 
        {
            "source": "<a href=#top>Back to Top</a>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Supplimentary codes", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# cleanup data\n\n# people with higher class survived more\nclass_pivot = df_train.pivot_table(index=\"Pclass\",values=\"Survived\")\nclass_pivot.plot.bar()\nplt.show()\n# Age Vs Survival ratio\nsurvived = df_train[df_train[\"Survived\"] == 1]\ndied = df_train[df_train[\"Survived\"] == 0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='red',bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5,color='blue',bins=50)\nplt.legend(['Survived','Died'])\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# explore data\n\n# we can also bucket categorical data into different features to avoid any new unseen values in future. \n# example: PCLASS have 3 types as of now. In future, if new feature comes in, model will get confused on how to train based on new type value.  \ndef process_embarked_feature(df,column_name):\n    embarked_dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,embarked_dummies],axis=1)\n    return df\n\ndf_train = process_embarked_feature(df_train,\"Embarked\")\ndf_practice = process_embarked_feature(df_practice,\"Embarked\")\ndf_train.dtypes\n\n# create new feature\ndef process_family(df):\n    # introducing a new feature : the size of families (including the passenger)\n    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\n    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n    return df\n\ndf_train=process_family(df_train)\ndf_practice=process_family(df_practice)\n\ndf_train.info()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# feature selection\n\n# check important features ranking\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectFromModel\n\nclf = LogisticRegression()\nclf=clf.fit(df_train[columns], df_train[\"Survived\"])\n\nrfe = RFE(clf, 3)\nrfe = rfe.fit(train_X, train_y)\n\nprint(rfe.support_)\nprint(rfe.ranking_)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# model validation\n\nfrom sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nscores = cross_val_score(lr, all_X, all_y, cv=10)\nscores.sort()\naccuracy = scores.mean()\n\nprint(scores)\nprint(accuracy)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# training accuracy\ntrainpredictions = lr.predict(train_X)\ntrainaccuracy = accuracy_score(train_y, trainpredictions)\ntrainaccuracy"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# learning curve\n!pip install mlxtend\nfrom mlxtend.plotting import plot_learning_curves\nplot_learning_curves(train_X, train_y, test_X, test_y, lr)\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# check feature importance graphically\nclf = LogisticRegression()\n# clf = clf.fit(train_X, train_y)\nX=df_train[columns]\nY=df_train[\"Survived\"]\nclf=clf.fit(X,Y)\n\nfeature_importance = clf.coef_[0]\n# feature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure()\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center')\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X.columns)[sorted_idx], fontsize=8)\nfeatax.set_xlabel('Relative Feature Importance')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.feature_selection import RFE\nrfe = RFE(clf, 5)\nrfe = rfe.fit(X, Y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\n# generate confusion matrix to see recall and precision details\nimport seaborn as sns\nfrom sklearn import metrics\n\ncfmap=metrics.confusion_matrix(y_true=test_y,  # True labels\n                         y_pred=predictions)\n\nlabel = [\"Died\", \"Survived\"]\nsns.heatmap(cfmap, annot = True, xticklabels = label, yticklabels = label)\nplt.xlabel(\"Prediction - logistic regression Algorithm\")\nplt.title(\"Confusion Matrix for logistic regression Algorithm\")\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# CELL TO LEAVE UNMODIFIED\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(test_y,predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nroc_auc"
        }, 
        {
            "source": "## Python basics sample", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# move it down\n# tuple\ntuple1=(\"disco\",10,1.2 )\nprint(tuple1)\nprint(type(tuple1))\nprint( type(tuple1[0]))\nprint( type(tuple1[1]))\nprint( type(tuple1[2]))\ntuple2=tuple1+(\"hard rock\", 10)\nprint(tuple2)\nNestedT =(1, 2, (\"pop\", \"rock\") ,(3,4),(\"disco\",(1,2)))\nprint(NestedT)\nprint(\"Element 0 of Tuple: \",   NestedT[0])\nprint(\"Element 1 of Tuple: \",  NestedT[1])\nprint(\"Element 2 of Tuple: \",  NestedT[2])\nprint(\"Element 3 of Tuple: \", NestedT[3])\nprint(\"Element 4 of Tuple: \", NestedT[4])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# list\nL=[\"Michael Jackson\" , 10.1,1982]\nprint('the same element using negative and positive indexing:\\n Postive:',L[0],\n'\\n Negative:' , L[-3]  )\nprint(L)\nL.extend(['pop',10])\nprint(L)\nL.append(['pop',10])\nprint(L)\ndel(L[5])\nL"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# string to list\nprint('hard rock'.split())\nprint('A,B,C,D'.split(','))\nB=A[:] # clone list\nB"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# dictionary\nDict={\"key1\":1,\"key2\":\"2\",\"key3\":[3,3,3],\"key4\":(4,4,4),('key5'):5,(0,1):6}\nDict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "release_year_dict = {\"Thriller\":\"1982\", \"Back in Black\":\"1980\", \\\n                    \"The Dark Side of the Moon\":\"1973\", \"The Bodyguard\":\"1992\", \\\n                    \"Bat Out of Hell\":\"1977\", \"Their Greatest Hits (1971-1975)\":\"1976\", \\\n                    \"Saturday Night Fever\":\"1977\", \"Rumours\":\"1977\"}\nprint(release_year_dict)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "release_year_dict['Thriller'] "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "release_year_dict.keys() "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "release_year_dict.values() "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "release_year_dict['Graduation']='2007'\nrelease_year_dict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "del(release_year_dict['Thriller'])\ndel(release_year_dict['Graduation'])\nrelease_year_dict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "'The Bodyguard' in release_year_dict"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# conditional statements\nage=18\nif age>18:\n    \n    print(\"you can enter\" )\nelif age==18:\n    print(\"go see Pink Floyd\")\nelse:\n    print(\"go see Meat Loaf\" )\n    \n\nprint(\"move on\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "album_year = 1980\n\nif(album_year > 1979) and (album_year < 1990):\n    print (\"Album year was in between 1981 and 1989\")\n    \nprint(\"\")\nprint(\"Do Stuff..\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "album_year = 1983\n\nif not (album_year == '1984'):\n    print (\"Album year is not 1984\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#loops\ndates = [1982,1980,1973]\nN=len(dates)\n\nfor i in range(N):\n     \n    print(dates[i]) \n\nfor year in dates:  \n    print(year)  "
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "squares=['red','yellow','green','purple','blue ']\n\nfor i in range(0,5):\n    print(\"Before square \",i, 'is',  squares[i])\n    \n    squares[i]='wight'\n    \n    print(\"After square \",i, 'is',  squares[i])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "squares=['red','yellow','green','purple','blue ']\n\nfor i,square in enumerate(squares): # access the index and the elements of a list\n    print(i,square)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "dates = [1982,1980,1973,2000]\n\ni=0;\nyear=0\nwhile(year!=1973):\n    year=dates[i]\n    i=i+1\n    print(year)\n    \n    \nprint(\"it took \", i ,\"repetitions to get out of loop\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# functions\ndef Mult(a,b):\n    c=a*b\n    return(c)\nMult(2,3)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def MJ():\n    print('Michael Jackson')\n    \ndef MJ1():\n    print('Michael Jackson')\n    return(None)\n\nprint(MJ())\nprint(MJ1())"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def type_of_album(artist,album,year_released):\n    if year_released > 1980:\n        print(artist,album,year_released)\n        return \"Modern\"\n    else:\n        print(artist,album,year_released)\n        return \"Oldie\"\n    \nx = type_of_album(\"Michael Jackson\",\"Thriller\",1980)\nprint(x)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def isGoodRating(rating=4): \n    if(rating < 7):\n        print(\"this album sucks it's rating is\",rating)\n        \n    else:\n        print(\"this album is good its rating is\",rating)\nisGoodRating()\nisGoodRating(10)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "artist = \"Michael Jackson\"\n\ndef printer(artist):\n    global internal_var \n    internal_var= \"Whitney Houston\"\n    print(artist,\"is an artist\")\n\nprinter(artist) \nprinter(internal_var)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}